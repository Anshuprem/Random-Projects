{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchcam'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m transforms, models\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchcam\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmethods\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GradCAM  \n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_activation_map\u001b[39m(model, input_tensor, target_layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer4\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Store the gradients\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchcam'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.fft import fft\n",
    "from skimage.feature import local_binary_pattern\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "from torchcam.methods import GradCAM  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_activation_map(model, input_tensor, target_layer='layer4'):\n",
    "    # Store the gradients\n",
    "    gradients = []\n",
    "    def save_gradient(grad):\n",
    "        gradients.append(grad)\n",
    "    \n",
    "    # Get the output of the target layer\n",
    "    for name, module in model.cnn.named_children():\n",
    "        input_tensor = module(input_tensor)\n",
    "        if name == target_layer:\n",
    "            input_tensor.register_hook(save_gradient)\n",
    "            layer_output = input_tensor\n",
    "    \n",
    "    # Get model output\n",
    "    output = model(input_tensor)\n",
    "    \n",
    "    # Calculate gradients\n",
    "    if len(gradients) > 0:\n",
    "        # Get the gradient for the target class\n",
    "        output.backward()\n",
    "        gradients = gradients[0].cpu().data.numpy()\n",
    "        weights = np.mean(gradients, axis=(2, 3))[0, :]\n",
    "        \n",
    "        # Generate heatmap\n",
    "        cam = np.zeros(layer_output.shape[2:], dtype=np.float32)\n",
    "        for i, w in enumerate(weights):\n",
    "            cam += w * layer_output[0, i].cpu().data.numpy()\n",
    "            \n",
    "        cam = np.maximum(cam, 0)\n",
    "        cam = (cam - np.min(cam)) / (np.max(cam) - np.min(cam) + 1e-7)\n",
    "        return cam\n",
    "    return None\n",
    "# ----------------------\n",
    "# 1. INITIALIZATION\n",
    "# ----------------------\n",
    "cap = cv2.VideoCapture(0)\n",
    "background_subtractor = cv2.createBackgroundSubtractorMOG2()\n",
    "intensity_history = []\n",
    "\n",
    "# Load HSV reference image\n",
    "hsv_reference = cv2.imread(\"hsv-0.png\") or np.zeros((100, 300, 3), dtype=np.uint8)\n",
    "\n",
    "# ----------------------\n",
    "# 2. FIRE DETECTION FUNCTIONS\n",
    "# ----------------------\n",
    "def fire_color_mask(frame):\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    # Adjusted HSV range for fire-like colors (red, orange, yellow)\n",
    "    lower_bound1 = np.array([0, 50, 50], dtype=np.uint8)   # Red hues\n",
    "    upper_bound1 = np.array([10, 255, 255], dtype=np.uint8)\n",
    "    lower_bound2 = np.array([160, 50, 50], dtype=np.uint8) # Red hues (wrap-around)\n",
    "    upper_bound2 = np.array([179, 255, 255], dtype=np.uint8)\n",
    "    lower_bound3 = np.array([18, 100, 100], dtype=np.uint8) # Orange-yellow hues\n",
    "    upper_bound3 = np.array([35, 255, 255], dtype=np.uint8)\n",
    "    \n",
    "    # Combine masks for red and orange-yellow hues\n",
    "    mask1 = cv2.inRange(hsv, lower_bound1, upper_bound1)\n",
    "    mask2 = cv2.inRange(hsv, lower_bound2, upper_bound2)\n",
    "    mask3 = cv2.inRange(hsv, lower_bound3, upper_bound3)\n",
    "    mask = cv2.bitwise_or(mask1, mask2)\n",
    "    mask = cv2.bitwise_or(mask, mask3)\n",
    "    \n",
    "    # Morphological operations to clean up the mask\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    # Find contours and filter based on size and shape\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) > 1000:  # Increased area threshold for better accuracy\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            aspect_ratio = float(w) / h\n",
    "            if 0.5 < aspect_ratio < 2.0:  # Filter based on aspect ratio to avoid false positives\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "                cv2.putText(frame, \"Fire Detected!\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "    return frame, mask\n",
    "\n",
    "    mask = cv2.bitwise_or(\n",
    "        cv2.inRange(hsv, lower1, upper1),\n",
    "        cv2.inRange(hsv, lower2, upper2)\n",
    "    )\n",
    "    mask = cv2.bitwise_or(mask, cv2.inRange(hsv, lower3, upper3))\n",
    "    \n",
    "    kernel = np.ones((5,5), np.uint8)\n",
    "    return cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "def detect_motion(frame):\n",
    "    fg_mask = background_subtractor.apply(frame)\n",
    "    return cv2.morphologyEx(fg_mask, cv2.MORPH_CLOSE, np.ones((5,5), np.uint8))\n",
    "\n",
    "def check_flicker(history, sample_rate=30):\n",
    "    if len(history) < sample_rate: return False\n",
    "    yf = fft(history[-sample_rate:])\n",
    "    dominant_freq = np.abs(np.fft.fftfreq(sample_rate, 1/sample_rate)[np.argmax(np.abs(yf))])\n",
    "    return 8 < dominant_freq < 12  # Fire flickers at ~10Hz\n",
    "\n",
    "def analyze_texture(roi_gray):\n",
    "    lbp = local_binary_pattern(roi_gray, 24, 3, method='uniform')\n",
    "    return np.histogram(lbp, bins=np.arange(27), density=True)[0]\n",
    "\n",
    "# ----------------------\n",
    "# 3. DEEP LEARNING MODEL\n",
    "# ----------------------\n",
    "class FireDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = models.resnet18(pretrained=True)\n",
    "        self.cnn.fc = nn.Identity()\n",
    "        self.lstm = nn.LSTM(512, 128, batch_first=True)\n",
    "        self.classifier = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.shape[0], x.shape[1]\n",
    "        features = self.cnn(x.view(-1, *x.shape[2:]))\n",
    "        _, (hidden, _) = self.lstm(features.view(batch_size, seq_len, -1))\n",
    "        return torch.sigmoid(self.classifier(hidden[-1]))\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = FireDetector().to(device)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ----------------------\n",
    "# 4. MAIN PROCESSING LOOP\n",
    "# ----------------------\n",
    "frame_buffer = []\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: break\n",
    "    \n",
    "    # Traditional Computer Vision\n",
    "    mask = fire_color_mask(frame)\n",
    "    motion_mask = detect_motion(frame)\n",
    "    gray = cv2.cvtColor(cv2.bitwise_and(frame, frame, mask=mask), cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Feature Extraction\n",
    "    intensity_history.append(np.mean(gray) if np.any(gray) else 0)\n",
    "    is_flickering = check_flicker(intensity_history)\n",
    "    texture_feat = analyze_texture(gray)\n",
    "    \n",
    "    # Deep Learning\n",
    "    frame_buffer.append(transform(frame))\n",
    "    if len(frame_buffer) == 30:  # Process every 1 sec (30fps)\n",
    "        with torch.no_grad():\n",
    "            prob = model(torch.stack(frame_buffer).unsqueeze(0).to(device)).item()\n",
    "        frame_buffer = []\n",
    "    \n",
    "    # Decision Fusion\n",
    "    cv_decision = (np.sum(motion_mask) > 1000 and is_flickering and np.sum(mask) > 5000)\n",
    "    dl_decision = prob > 0.7 if len(frame_buffer)==0 else False\n",
    "    \n",
    "    if cv_decision or dl_decision:\n",
    "        x,y,w,h = cv2.boundingRect(cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0][0])\n",
    "        cv2.rectangle(frame, (x,y), (x+w,y+h), (0,0,255), 2)\n",
    "        cv2.putText(frame, \"FIRE DETECTED\", (x,y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,255), 2)\n",
    "    \n",
    "    # Display\n",
    "    cv2.imshow(\"Live\", frame)\n",
    "    cv2.imshow(\"Fire Mask\", mask)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (356571872.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[18], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    python -c \"import torchcam; print(torchcam.__version__)\"\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python -c \"import torchcam; print(torchcam.__version__)\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
